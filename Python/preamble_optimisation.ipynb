{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the function to optimize (correlation mask example)\n",
    "target = np.array([1, 0, -1, 0, 1])  # Example target pattern\n",
    "def objective_function(mask):\n",
    "    target_noise = np.random.rand(len(target))/50\n",
    "    # Example correlation computation (replace with actual computation)\n",
    "    target_to_correlate = target + target_noise\n",
    "    correlation = np.correlate(mask, target_to_correlate, mode='valid')\n",
    "    return -np.max(correlation)  # Maximize correlation by minimizing its negative\n",
    "\n",
    "# Track optimization progress\n",
    "class OptimizationTracker:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def callback(self, xk):\n",
    "        self.history.append(xk)\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = OptimizationTracker()\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = np.random.rand(5)  # Example initial mask of size 5\n",
    "\n",
    "# Perform optimization using Nelder-Mead\n",
    "result = minimize(\n",
    "    objective_function,\n",
    "    initial_guess,\n",
    "    method='Nelder-Mead',\n",
    "    callback=tracker.callback,\n",
    "    options={'disp': True}\n",
    ")\n",
    "\n",
    "# Print optimization results\n",
    "print(\"\\nOptimization Result:\")\n",
    "print(result)\n",
    "\n",
    "# Extract data for plotting\n",
    "history = np.array(tracker.history)\n",
    "z_vals = np.array([objective_function(x) for x in history])\n",
    "\n",
    "\n",
    "# Plot evolution of the objective function\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(result.x/max(result.x), label=\"optimized\")\n",
    "plt.plot(target, label=\"og\")\n",
    "plt.title(\"Optimized Mask\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(212)\n",
    "plt.plot(z_vals, marker='o', label=\"Objective Value\")\n",
    "plt.title(\"Objective Function Evolution\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective Function Value\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from gymnasium import Env, spaces\n",
    "\n",
    "\n",
    "# Define a custom environment inheriting from gymnasium.Env\n",
    "class CustomEnv(Env):\n",
    "    def __init__(self, config: EnvContext):\n",
    "        super().__init__()\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(2)  # Actions: 0 or 1\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        return np.array([self.state]), {}  # Ensure observation is a numpy array\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Step called after environment is done\")\n",
    "\n",
    "        self.state += 1 if action == 1 else -1\n",
    "        terminated = abs(self.state) >= 5\n",
    "        truncated = False\n",
    "        reward = 1 if terminated and self.state > 0 else 0\n",
    "\n",
    "        return np.array([self.state]), reward, terminated, truncated, {}\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        pass\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "class CustomMetrics(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        # Log custom metrics like episode rewards\n",
    "        total_reward = episode.total_reward\n",
    "        print(f\"Episode finished! Total reward: {total_reward}\")\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Register the custom environment\n",
    "register_env(\"custom_env\", lambda config: CustomEnv(config))\n",
    "\n",
    "# Configuration for the RLlib PPO trainer\n",
    "config = {\n",
    "    \"env\": \"custom_env\",\n",
    "    \"env_config\": {},\n",
    "    \"framework\": \"torch\",\n",
    "    \"callbacks\": CustomMetrics,\n",
    "}\n",
    "\n",
    "# Initialize and train PPO\n",
    "trainer = PPO(config=config)\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    result = trainer.train()\n",
    "    # print(f\"Iteration: {i}, Reward Mean: {result['episode_reward_mean']}\")\n",
    "\n",
    "# Clean up\n",
    "trainer.stop()\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
